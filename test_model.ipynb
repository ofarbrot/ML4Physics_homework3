{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840769d9",
   "metadata": {},
   "source": [
    "# Training and testing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59391172",
   "metadata": {},
   "source": [
    "### Importing relevant Liberaries, Functions and Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54007b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "from helpers_HW.HW3.ion_trap import IonTrapEnv\n",
    "from helpers_HW.HW3.utils import is_valid_srv\n",
    "from model import model\n",
    "from train import train_ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03137fa",
   "metadata": {},
   "source": [
    "### Defining Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3e162d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the SRV defines the entanglement of the goal state\n",
    "srv = [3,3,3]\n",
    "\n",
    "KWARGS = {'phases': {'pulse_angles': [np.pi/2], 'pulse_phases': [np.pi/2], 'ms_phases': [-np.pi/2]}, # Gates available\n",
    "          'num_ions': 3, \n",
    "          'goal': [srv],\n",
    "          'max_steps': 10 # This is already default for the environment\n",
    "         } \n",
    "\n",
    "env = IonTrapEnv(**KWARGS)\n",
    "\n",
    "#agent = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc22573",
   "metadata": {},
   "source": [
    "### Training and Evaluating Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a7b46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'IonTrapEnv' object has no attribute 'observation_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m steps, rewards = [], []\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(runs)):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     _, _, time_steps, obtained_rewards = \u001b[43mtrain_ps\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# _, _, time_steps, obtained_rewards = train_ps_from_lib(gamma, eta, episodes)\u001b[39;00m\n\u001b[32m     10\u001b[39m     steps.append(time_steps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oskar\\ML_course_homeworks\\ML4Physics_homework3\\train.py:6\u001b[39m, in \u001b[36mtrain_ps\u001b[39m\u001b[34m(env, gamma, eta, episodes)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_ps\u001b[39m(env, gamma, eta, episodes):\n\u001b[32m      5\u001b[39m     ps_agent = PS_agent(gamma=gamma, eta=eta, \n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m                         num_states=\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobservation_space\u001b[49m.n, \n\u001b[32m      7\u001b[39m                         num_actions=env.num_actions)\n\u001b[32m      9\u001b[39m     max_t = \u001b[32m99\u001b[39m    \n\u001b[32m     10\u001b[39m     time_steps = []\n",
      "\u001b[31mAttributeError\u001b[39m: 'IonTrapEnv' object has no attribute 'observation_space'"
     ]
    }
   ],
   "source": [
    "gamma = 1e-7; eta = 0.006\n",
    "\n",
    "runs = 30\n",
    "episodes = 500\n",
    "steps, rewards = [], []\n",
    "for i in tqdm(range(runs)):\n",
    "    _, _, time_steps, obtained_rewards = train_ps(env, gamma, eta, episodes)\n",
    "    # _, _, time_steps, obtained_rewards = train_ps_from_lib(gamma, eta, episodes)\n",
    "\n",
    "    steps.append(time_steps)\n",
    "    rewards.append(obtained_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f275e",
   "metadata": {},
   "source": [
    "### Testing submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b84747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_srv = None\n",
    "agent = model()\n",
    "preds = agent.pred(valid_srv)\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
