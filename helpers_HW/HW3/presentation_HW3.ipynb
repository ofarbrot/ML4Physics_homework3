{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7befe681-62e2-4e66-905b-8332b48b0762",
   "metadata": {},
   "source": [
    "# HW 3 - Quantum state preparation with RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194efc6b-b712-4b0e-9fb8-37bfb1bc7fe1",
   "metadata": {},
   "source": [
    "For this homework we will make use of the Ion Trap environment developed by Hendrik Poulsen Nautrup [here](https://github.com/HendrikPN/rl-ion-trap-tutorial). The environment simulates the preparation of quantum states in an qudit ion trap quantum computer using a restricted set of quantum gates. The goal is to prepare a specific target state with a given Schmidt Rank vector (SRV), starting from an initial state by applying a sequence of quantum gates (actions). Let's start by defining the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3d1dccb-65b6-4f9e-b468-63ba195c8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ion_trap import IonTrapEnv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# the SRV defines the entanglement of the goal state\n",
    "srv = [3,3,3]\n",
    "\n",
    "KWARGS = {'phases': {'pulse_angles': [np.pi/2], 'pulse_phases': [np.pi/2], 'ms_phases': [-np.pi/2]}, # Gates available\n",
    "          'num_ions': 3, \n",
    "          'goal': [srv],\n",
    "          'max_steps': 10 # This is already default for the environment\n",
    "         } \n",
    "\n",
    "env = IonTrapEnv(**KWARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79450dba-b98c-40e3-95f6-2dec19528578",
   "metadata": {},
   "source": [
    "This environment has a discrete action space, where each action corresponds to applying a specific quantum gate to the current state. The number of possible actions is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d3d101-ed32-4cdd-9b6e-ebd09774a3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.num_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4d36e4-49cf-43c2-9476-327af0b34e69",
   "metadata": {},
   "source": [
    "To perform an action, we just use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "755efa8c-c2d9-4d18-ada2-ba83366f66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 5\n",
    "observation, reward, done = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c9e43e-5468-4b58-9fb8-6ecca76edf32",
   "metadata": {},
   "source": [
    "The observation is the current state of the quantum system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "894ee87f-bbaa-45a7-9d3f-9676870ea74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.+0.000000e+00j],\n",
       "       [-1.-6.123234e-17j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j],\n",
       "       [ 0.+0.000000e+00j]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16b0d77-df23-4990-9914-8e784d754043",
   "metadata": {},
   "source": [
    "The reward is 1 if the goal state is reached. On the other hand, `done` is `True` if the episode has ended, either because the goal state has been reached or because the maximum number of steps has been taken.\n",
    "\n",
    "For this homework, we will consider an enhanced version of this problem, and train an agent to prepare quantum states with different target SRVs. The agent will receive as input both the current state of the quantum system and the target SRV, and will have to learn a policy that can generalize across different target states. We will consider the SRVs given by the function `is_valid_srv` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b836c326-f107-4f7e-97be-bad07d1a797d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [1, 2, 2],\n",
       "        [2, 1, 2],\n",
       "        [2, 2, 1],\n",
       "        [2, 2, 2],\n",
       "        [2, 2, 3],\n",
       "        [2, 3, 2],\n",
       "        [2, 3, 3],\n",
       "        [3, 2, 2],\n",
       "        [3, 2, 3],\n",
       "        [3, 3, 2],\n",
       "        [3, 3, 3]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import is_valid_srv\n",
    "from itertools import product\n",
    "import torch\n",
    "\n",
    "all_srv = list(product([1,2,3], repeat = env.num_ions))\n",
    "\n",
    "valid_srv = torch.tensor([srv for srv in all_srv if srv == (1, 1, 1) or is_valid_srv(srv)[0]])\n",
    "valid_srv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea411ce-81c1-4c2c-a725-00bd1e7a3c0b",
   "metadata": {},
   "source": [
    "## Dummy implementation\n",
    "Your submission to Codabench is just as before, you will need to submit a model that has to give a gate sequence for the given SRVs. Here is a dummy implementation of how it should work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6de46d8-d816-4c8b-a57a-0d6e9e7dd934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass    \n",
    "    \n",
    "    def pred(self, samples):\n",
    "\n",
    "        # This agent will just create random gate sequences        \n",
    "        preds = []\n",
    "        for s in samples:\n",
    "            num_gates = torch.randint(3,10,(1,))\n",
    "            gate_sequence = torch.randint(0, 7, (num_gates,))\n",
    "            preds.append(gate_sequence)\n",
    "            \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d71146-8484-4d6d-ae6d-80a2b40f7ec4",
   "metadata": {},
   "source": [
    "Then, the ingestion program in Codabench will run the following code to evaluate your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74ae1784-71ef-4852-ba76-67066e15a2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([2, 2, 0, 3, 5, 6]),\n",
       " tensor([2, 4, 3, 5, 1, 5]),\n",
       " tensor([4, 0, 3, 5]),\n",
       " tensor([3, 0, 2, 2]),\n",
       " tensor([3, 3, 5, 5, 1, 3, 0, 4, 5]),\n",
       " tensor([2, 0, 6, 2, 1, 6, 2]),\n",
       " tensor([5, 2, 3, 0, 1, 1, 1]),\n",
       " tensor([0, 6, 0, 3, 3, 5, 3, 3, 4]),\n",
       " tensor([0, 4, 6]),\n",
       " tensor([5, 0, 5, 5, 5, 6, 2]),\n",
       " tensor([1, 0, 4]),\n",
       " tensor([6, 1, 6, 6, 6, 1, 4])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = model()\n",
    "preds = agent.pred(valid_srv)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382727ee-6b61-4927-8bd0-f7a764a2ffc7",
   "metadata": {},
   "source": [
    "From here, the scoring program will evaluate your predictions just as below. The score in this homework is the average gate length achieved by your agent (i.e. smaller is better). However, only gate sequences that lead to correct target states (`reward == 1`) will count, and any other will be penalized with the maximum gate length allowed by the environment (10 in this case).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37116a04-0c79-4995-8fb2-d04784f65f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(10.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = []\n",
    "for srv, pred in zip(valid_srv, preds):\n",
    "\n",
    "    KWARGS = {'phases': {'pulse_angles': [np.pi/2], 'pulse_phases': [np.pi/2], 'ms_phases': [-np.pi/2]}, 'num_ions': 3, 'goal': [list(srv)]}\n",
    "    env = IonTrapEnv(**KWARGS) \n",
    "    \n",
    "    env.reset()\n",
    "    for action in pred:\n",
    "        # perform action on environment and receive observation and reward\n",
    "        observation, reward, done = env.step(action)\n",
    "        # print(idx_srv, action, reward)\n",
    "    if reward == 1:\n",
    "        seq_length.append(len(pred))\n",
    "    else:\n",
    "        seq_length.append(env.max_steps)\n",
    "\n",
    "# The random agent is very bad, hence the score being 10!\n",
    "np.mean(seq_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
