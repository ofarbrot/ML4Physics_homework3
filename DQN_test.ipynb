{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44531cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "from helpers_HW.HW3.ion_trap import IonTrapEnv\n",
    "from helpers_HW.HW3.utils import is_valid_srv\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, s, a, r, s2, done):\n",
    "        # Stores new values in the buffer\n",
    "        self.buffer.append((s, a, r, s2, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # Samples a batch of experiences from the buffer\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s, a, r, s2, d = map(np.array, zip(*batch))\n",
    "        return (\n",
    "            torch.tensor(s, dtype=torch.float32, device=device),\n",
    "            torch.tensor(a, dtype=torch.int64,   device=device),\n",
    "            torch.tensor(r, dtype=torch.float32, device=device),\n",
    "            torch.tensor(s2, dtype=torch.float32,device=device),\n",
    "            torch.tensor(d, dtype=torch.float32, device=device),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "   \n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, n_actions,\n",
    "                 # Q update parameter\n",
    "                 gamma=0.99, \n",
    "                 # Learning rate\n",
    "                 lr=1e-3, \n",
    "                 # Parameters for epsilon decay\n",
    "                 eps_start=1.0, eps_end=0.05, eps_decay=500):\n",
    "       \n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.q_net = QNetwork(state_dim, n_actions).to(device)\n",
    "\n",
    "        # And now the target network, which is just a copy of the previous one.\n",
    "        # Because this one won't be trained, we directly set it in eval mode!\n",
    "        self.target_net = QNetwork(state_dim, n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "\n",
    "        # Parameters for epsilon decay\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_decay = eps_decay\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def epsilon(self):\n",
    "        # Computes the epsilon value based on the decay schedule\n",
    "        return self.eps_end + (self.eps_start - self.eps_end) * \\\n",
    "               np.exp(-1.0 * self.steps_done / self.eps_decay)\n",
    "    \n",
    "    def act(self, state): #obs_np: np.ndarray\n",
    "        # We will start by increasing the step counter for the epsilon decay\n",
    "        self.steps_done += 1\n",
    "        eps = self.epsilon()\n",
    "        \n",
    "        # Important: because here there won't be any policy update, we can perform the sampling\n",
    "        # from the q-network with torch.no_grad():\n",
    "        if random.random()<eps:\n",
    "            action = random.randrange(self.n_actions)\n",
    "\n",
    "        # random step\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                obs = torch.tensor(obs_np, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                q = self.q_net(obs)               # [1, n_actions]\n",
    "                action = int(torch.argmax(q, dim=1).item())\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update(self, replay_buffer, batch_size):\n",
    "    \n",
    "            # If our buffer does not have enough samples to fill a full batch, we skip the update\n",
    "            if len(replay_buffer) < batch_size:\n",
    "                return\n",
    "    \n",
    "            # If not, we sample a batch from the replay buffer\n",
    "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "            # We now sample from a q network. Because we are interested only in the Q-values of the actions taken,\n",
    "            # we use the `gather` method of pytorch tensors to select the Q-values corresponding to the actions taken.\n",
    "            q_values = self.q_net(states)\n",
    "            q_sa = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                max_next_q = self.target_net(next_states).max(dim=1).values\n",
    "                target = rewards + self.gamma * (1.0 - dones) * max_next_q\n",
    "\n",
    "            loss = nn.MSELoss()(q_sa, target)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.q_net.parameters(), 5.0)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Now implement the same as above considering the previous batches\n",
    "\n",
    "    # Finally, we define the target network update method, which simply copies the weights from the Q-network to the target network.\n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc22bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1], [1, 2, 2], [2, 1, 2], [2, 2, 1], [2, 2, 2], [2, 2, 3], [2, 3, 2], [2, 3, 3], [3, 2, 2], [3, 2, 3], [3, 3, 2], [3, 3, 3]]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "def all_valid_srvs(num_ions: int, dim: int, is_valid_srv_fn):\n",
    "    valids = []\n",
    "    for srv in product(range(1, dim+1), repeat=num_ions):\n",
    "        ok, _ = is_valid_srv_fn(srv, d=dim)\n",
    "        if ok:\n",
    "            valids.append(list(srv))\n",
    "    return valids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23067ead",
   "metadata": {},
   "source": [
    "### Training etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa87ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_iontrap(\n",
    "    episodes=20000,\n",
    "    buffer_capacity=50000,\n",
    "    batch_size=128,\n",
    "    target_update_freq=500,\n",
    "    train_every=1,\n",
    "    seed=0,\n",
    "):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    env = IonTrapEnv()\n",
    "    n_actions = env.num_actions\n",
    "\n",
    "\n",
    "    valid_goals = all_valid_srvs(env.num_ions, env.dim, is_valid_srv)\n",
    "    if len(valid_goals) == 0:\n",
    "        raise RuntimeError(\"No valid SRVs found for this (num_ions, dim).\")\n",
    "\n",
    "    # Observation dim = 2*(d^n) + num_ions\n",
    "    #state_dim = env.dim ** env.num_ions\n",
    "    #obs_dim = 2 * state_dim + env.num_ions\n",
    "\n",
    "    agent = DQNAgent(state_dim=, n_actions=n_actions,\n",
    "                     gamma=0.99, lr=1e-3,\n",
    "                     eps_start=1.0, eps_end=0.05, eps_decay=20000)\n",
    "    replay = ReplayBuffer(capacity=buffer_capacity)\n",
    "\n",
    "    total_steps = 0\n",
    "    returns = []\n",
    "    success_rate_window = deque(maxlen=200)\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        goal = random.choice(valid_goals)\n",
    "\n",
    "\n",
    "        env.goal = [goal]  # env expects list of SRVs\n",
    "\n",
    "        state = env.reset()\n",
    "        obs = make_obs(state, goal, env)\n",
    "\n",
    "        ep_return = 0.0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(obs)\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            next_obs = make_obs(next_state, goal, env)\n",
    "\n",
    "            replay.push(obs, action, reward, next_obs, float(done))\n",
    "\n",
    "            obs = next_obs\n",
    "            ep_return += reward\n",
    "            total_steps += 1\n",
    "\n",
    "            if total_steps % train_every == 0:\n",
    "                agent.update(replay, batch_size)\n",
    "\n",
    "            if total_steps % target_update_freq == 0:\n",
    "                agent.update_target()\n",
    "\n",
    "        returns.append(ep_return)\n",
    "        success_rate_window.append(1 if ep_return > 0.0 else 0)\n",
    "\n",
    "        # print progress occasionally\n",
    "        if (ep + 1) % 500 == 0:\n",
    "            sr = sum(success_rate_window) / len(success_rate_window)\n",
    "            print(f\"Episode {ep+1:6d} | recent success rate: {sr:.3f} | eps: {agent.epsilon():.3f}\")\n",
    "\n",
    "    return agent, returns\n",
    "\n",
    "# Example:\n",
    "agent, returns, valid_goals = train_dqn_iontrap(episodes=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d0aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_dqn_iontrap(\n",
    "    episodes=300):\n",
    "\n",
    "    # Initialized the environment\n",
    "    env = IonTrapEnv()\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    # Initialize the DQN agent and buffer\n",
    "    agent = DQNAgent(state_dim, n_actions)\n",
    "    batch_size=64\n",
    "    target_update_freq=1000\n",
    "\n",
    "    replay = ReplayBuffer(capacity = 50000)\n",
    "\n",
    "    # We keep track of the number of total steps for the target network update\n",
    "    total_steps = 0\n",
    "\n",
    "    # We keep track of the amount of rewards\n",
    "    returns = []  \n",
    "\n",
    "    pbar = trange(episodes, desc=\"Training DQN\", leave=True)\n",
    "\n",
    "    for ep in pbar:\n",
    "        \n",
    "        s, _ = env.reset() \n",
    "        ep_return = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # act\n",
    "            a = agent.act(s)\n",
    "            \n",
    "            s2, r, terminated, truncated, _ = env.step(a)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # store transition\n",
    "            replay.push(s, a, r, s2, done)\n",
    "            s = s2\n",
    "            ep_return += r\n",
    "            total_steps += 1\n",
    "\n",
    "            # update networks\n",
    "            agent.update(replay, batch_size)\n",
    "\n",
    "            if total_steps % target_update_freq == 0:\n",
    "                agent.update_target()\n",
    "\n",
    "        # logging\n",
    "        returns.append(ep_return)\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "            return_=f\"{ep_return:.1f}\",\n",
    "            eps=f\"{agent.epsilon():.3f}\"\n",
    "        )\n",
    "\n",
    "    return agent, returns\n",
    "\n",
    "plt.plot(returns)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.title(\"DQN on CartPole\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
